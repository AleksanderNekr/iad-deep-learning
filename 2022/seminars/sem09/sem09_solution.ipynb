{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "colab": {
   "name": "sem08_solution.ipynb",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1W8R8WgZceEk"
   },
   "source": [
    "# Семинар 9: Character-Level LSTM\n",
    "\n",
    "## Вступление\n",
    "На прошлом занятии мы познакомились с тем, как можно векторизовать текстовые данные для решения задач обработки текстов. Сегодня мы продолжим заниматься текстами и посмотрим на простейший пример автоматической генерации текстов при помощи Recurrent Neural Network (RNN).\n",
    "\n",
    "Полезные материалы по RNN можно почитать [здесь]((http://karpathy.github.io/2015/05/21/rnn-effectiveness/)), а реализацию на PyTorch — [здесь](https://github.com/karpathy/char-rnn).\n",
    "\n",
    "### План семинара\n",
    "1. Подготовка данных\n",
    "2. Имплементация модели\n",
    "3. Обучение модели\n",
    "4. Применение модели"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sqUOE2flceEl"
   },
   "source": [
    "from typing import Iterable, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wHfCDyzceEl"
   },
   "source": [
    "## 1. Подготовка данных\n",
    "\n",
    "### Загрузим текст \"Анны Карениной\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b34kfqIOceEl"
   },
   "source": [
    "with open(\"anna.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text[:100]"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iC21bopceEl"
   },
   "source": [
    "### Токенизируем текст\n",
    "\n",
    "Аналогично предыдущему семинару, в ячейках ниже создадим два **словаря** для преобразования символов в целые числа и обратно."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tYVlmnxLceEl"
   },
   "source": [
    "unique_chars = tuple(set(text))\n",
    "int2char = dict(enumerate(unique_chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "encoded[:100]"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "array([15, 76, 14, 56, 68, 46, 71, 80, 70, 72, 72, 72,  3, 14, 56, 56, 67,\n       80, 55, 14, 69,  1, 45,  1, 46, 66, 80, 14, 71, 46, 80, 14, 45, 45,\n       80, 14, 45,  1, 74, 46, 52, 80, 46, 23, 46, 71, 67, 80, 59, 44, 76,\n       14, 56, 56, 67, 80, 55, 14, 69,  1, 45, 67, 80,  1, 66, 80, 59, 44,\n       76, 14, 56, 56, 67, 80,  1, 44, 80,  1, 68, 66, 80, 28,  7, 44, 72,\n        7, 14, 67, 48, 72, 72,  2, 23, 46, 71, 67, 68, 76,  1, 44])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azltQy-gceEl"
   },
   "source": [
    "Посмотрим на схему char-RNN:\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charseq.jpeg?raw=1\" width=\"30%\">\n",
    "\n",
    "Сеть ожидает **one-hot encoded** входа, что означает, что каждый символ преобразуется в целое число (через созданный маппинг), а затем преобразуется в вектор-столбец, где только соответствующий ему целочисленный индекс будет иметь значение 1, а остальная часть вектора будет заполнена нулями. Давайте напишем функцию для этого преобразования.\n",
    "\n",
    "#### Задание: допишите функцию one-hot кодирования последовательности"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OnahALhiceEl"
   },
   "source": [
    "def one_hot_encode(int_words: np.array, n_labels: int) -> np.array:\n",
    "    \"\"\"\n",
    "    Creates one-hot representation matrix for a given batch of integer sequences\n",
    "    :param int_words: array of ints, which represents current sequence; shape: [batch_size, seq_len]\n",
    "    :param n_labels: vocabulary size (number of unique tokens in data)\n",
    "    :return: one-hot representation of the input array; shape: [batch_size, seq_len, n_labels]\n",
    "    \"\"\"\n",
    "    words_one_hot = np.zeros((int_words.size, n_labels), dtype=np.float32)\n",
    "    words_one_hot[np.arange(words_one_hot.shape[0]), int_words.flatten()] = 1.\n",
    "    words_one_hot = words_one_hot.reshape((*int_words.shape, n_labels))\n",
    "\n",
    "    return words_one_hot"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# testing the function\n",
    "test_seq = np.array([[3, 5, 1], [0, 2, 4]])\n",
    "test_one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(test_one_hot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YyL91CuceEl"
   },
   "source": [
    "### Сформируем батчи\n",
    "На простом примере батчи будут выглядеть так: мы возьмем закодированные символы и разделим их на несколько последовательностей, заданных параметром `batch_size`. Каждая из наших последовательностей будет иметь длину `seq_length`.\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/sequence_batching@1x.png?raw=1\" width=500px>\n",
    "\n",
    "**1. Отбросим часть текста, чтобы у нас были только полные батчи**\n",
    "\n",
    "Каждый батч содержит $N \\times M$ символов, где $N$ — это количество последовательностей в батче (`batch_size`), а $M$ — длина каждой последовательности (`seq_length`). Затем, чтобы получить общее количество батчей $K$, которое мы можем сделать из последовательности, нужно разделить длину последовательности на количество символов в батче. Когда мы узнаем количество батчей, можно получить общее количество символов, которые нужно сохранить, из последовательности: $N \\times M \\times K$.\n",
    "\n",
    "**2. Разделим текст на $N$ частей**\n",
    "\n",
    "Этот шаг нужен, чтобы мы могли проходить по тексту окном размера `[batch_size, seq_len]`. Его можно реализовать при помощи простого `reshape`.\n",
    "\n",
    "**3. Теперь, когда у нас готова матрица текста, мы можем двигаться по ней окном его, чтобы получить батчи**\n",
    "\n",
    "Из каждой позиции окна сформируем обучающие пары `(x, y)` следующим образом: $x$ — это все элементы окна кроме последнего столбца, а $y$ — это все элементы окна кроме первого столбца. Тем самым для каждого токена исходного текста мы будем предсказывать следующий за ним токен.\n",
    "\n",
    "#### Задание: допишите функцию генерации батчей"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2ECftYejnvpx"
   },
   "source": [
    "def get_batches(int_words: np.array, batch_size: int, seq_length: int) -> Iterable[np.array]:\n",
    "    \"\"\"\n",
    "    Generates batches from encoded sequence.\n",
    "    :param int_words: array of ints, which represents the text; shape: [batch_size, -1]\n",
    "    :param batch_size: number of sequences per batch\n",
    "    :param seq_length: number of encoded chars in a sequence\n",
    "    :return: generator of pairs (x, y); x_shape, y_shape: [batch_size, seq_length - 1]\n",
    "    \"\"\"\n",
    "    # 1. Truncate text, so there are only full batches\n",
    "    window_size = seq_length + 1\n",
    "    batch_size_total = batch_size * window_size\n",
    "    n_batches = len(int_words) // batch_size_total\n",
    "    int_words = int_words[:n_batches * batch_size_total]\n",
    "\n",
    "    # 2. Reshape into batch_size rows\n",
    "    int_words = int_words.reshape((batch_size, -1))\n",
    "    \n",
    "    # 3. Iterate through the text matrix\n",
    "    for position in range(0, int_words.shape[1], window_size):\n",
    "        x = int_words[:, position:position + window_size - 1]\n",
    "        y = int_words[:, position + 1:position + window_size]\n",
    "        # try:\n",
    "        #     y[:, :-1], y[:, -1] = x[:, 1:], int_words[:, position + seq_length]\n",
    "        # except IndexError:\n",
    "        #     y[:, :-1], y[:, -1] = x[:, 1:], int_words[:, 0]\n",
    "        yield x, y"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qtKlLXi1ceEl"
   },
   "source": [
    "# testing the function\n",
    "test_batches = get_batches(encoded, 8, 50)\n",
    "test_x, test_y = next(test_batches)\n",
    "assert test_x.shape == test_y.shape\n",
    "print(f\"x:\\n{test_x[:10, :10]}\\n\")\n",
    "print(f\"y:\\n{test_y[:10, :10]}\")"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "[[15 76 14 56 68 46 71 80 70 72]\n",
      " [56 56 46 58 80 14 44 58 80 66]\n",
      " [68 76 14 68 80  1 44 66 28 45]\n",
      " [46  7 80 68 76 28 59 39 76 68]\n",
      " [46 71 63 80 76 46 80 76 14 58]\n",
      " [71 63 80  7 76 28 80  7 14 66]\n",
      " [66 68 80 13 46 80 34 28 23 46]\n",
      " [63 80 25 45 46 77 46 67 80 25]]\n",
      "\n",
      "y:\n",
      "[[76 14 56 68 46 71 80 70 72 72]\n",
      " [56 46 58 80 14 44 58 80 66 76]\n",
      " [76 14 68 80  1 44 66 28 45 59]\n",
      " [ 7 80 68 76 28 59 39 76 68 55]\n",
      " [71 63 80 76 46 80 76 14 58 80]\n",
      " [63 80  7 76 28 80  7 14 66 80]\n",
      " [68 80 13 46 80 34 28 23 46 71]\n",
      " [80 25 45 46 77 46 67 80 25 45]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jouxv0L2ceEl"
   },
   "source": [
    "## 2. Имплементация модели\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charRNN.png?raw=1\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7s5eRaoceEl"
   },
   "source": [
    "### Структура модели\n",
    "\n",
    "* Создаём и храним необходимые словари.\n",
    "* Определяем слой [LSTM]((https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM)) с помощью инстанса класса `torch.nn.LSTM`, который принимает набор параметров: `input_size` — длина последовательности в батче; `n_hidden` — размер скрытых слоёв; `n_layers` — количество слоёв; `drop_prob` — вероятность дропаута; и `batch_first` — флаг, указывающий на то, что у входных последовательностей размерность батча идёт вдоль нулевой оси.\n",
    "* Определяем слой Dropout с таким же значением `drop_prob`.\n",
    "* Определяем полносвязный слой с набором параметров: размерность ввода — `n_hidden`; размерность выхода — размер словаря.\n",
    "* Наконец, инициализируем веса и начальное скрытое состояние (`self.init_hidden()`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VPq1EA38rBqn"
   },
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unique_tokens: Tuple[str],\n",
    "        n_hidden: int = 256,\n",
    "        n_layers: int = 2,\n",
    "        drop_prob: float = 0.5,\n",
    "        lr: float = 0.001\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating mappings\n",
    "        self.unique_tokens = unique_tokens\n",
    "        self.int2char = dict(enumerate(self.unique_tokens))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the LSTM, dropout and fully connected layers\n",
    "        self.lstm = nn.LSTM(len(self.unique_tokens), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.unique_tokens))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, hidden: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        # Stack up LSTM outputs using view. You may need to use contiguous to reshape the output.\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        ## Get the output for classification.\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size: int, weight_device: torch.device) -> Tuple[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Creates two new zero tensors for hidden state and cell state of LSTM\n",
    "        :param batch_size: number of sequences per batch\n",
    "        :param weight_device: torch.device(\"cuda\") for GPU init or torch.device(\"cpu\") for CPU init\n",
    "        :return: tuple of two tensors of shape [n_layers x batch_size x n_hidden]\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(weight_device),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(weight_device))\n",
    "        return hidden"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IrBRlEPceEl"
   },
   "source": [
    "## 3. Обучение модели\n",
    "\n",
    "По классике, используем оптимизатор Adam и кросс-энтропию. Но без пары особенностей не обойтись:\n",
    "* Во время цикла будем отделять скрытое состояние от его истории, потому что скрытое состояние LSTM, является кортежем скрытых состояний.\n",
    "* Будем использовать [`clip_grad_norm_`](https://pytorch.org/docs/stable/_modules/torch/nn/utils/clip_grad.html), чтобы избавиться от взрывающихся градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lv8VkRI0ceEl"
   },
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    net.to(device)\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.unique_tokens)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size, device)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x).to(device), torch.from_numpy(y).to(device)\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.reshape(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size, device)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    inputs, targets = torch.from_numpy(x).to(device), torch.from_numpy(y).to(device)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.reshape(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train()\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ],
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ykMcIloEr3G7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "300411a6-9dc4-4f34-aa4a-267f4769342a"
   },
   "source": [
    "net = CharRNN(unique_chars, n_hidden=32, n_layers=2)\n",
    "train(net, encoded, epochs=20, batch_size=128, seq_length=100, lr=0.001, print_every=10)"
   ],
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 4.3006... Val Loss: 4.2894\n",
      "Epoch: 1/20... Step: 20... Loss: 4.1379... Val Loss: 4.1019\n",
      "Epoch: 1/20... Step: 30... Loss: 3.7318... Val Loss: 3.6390\n",
      "Epoch: 1/20... Step: 40... Loss: 3.4612... Val Loss: 3.3511\n",
      "Epoch: 1/20... Step: 50... Loss: 3.3512... Val Loss: 3.2270\n",
      "Epoch: 1/20... Step: 60... Loss: 3.2861... Val Loss: 3.1733\n",
      "Epoch: 1/20... Step: 70... Loss: 3.2779... Val Loss: 3.1501\n",
      "Epoch: 1/20... Step: 80... Loss: 3.2537... Val Loss: 3.1387\n",
      "Epoch: 1/20... Step: 90... Loss: 3.2321... Val Loss: 3.1329\n",
      "Epoch: 1/20... Step: 100... Loss: 3.1972... Val Loss: 3.1301\n",
      "Epoch: 1/20... Step: 110... Loss: 3.2012... Val Loss: 3.1280\n",
      "Epoch: 1/20... Step: 120... Loss: 3.2104... Val Loss: 3.1264\n",
      "Epoch: 1/20... Step: 130... Loss: 3.1765... Val Loss: 3.1251\n",
      "Epoch: 2/20... Step: 140... Loss: 3.1581... Val Loss: 3.1241\n",
      "Epoch: 2/20... Step: 150... Loss: 3.1716... Val Loss: 3.1237\n",
      "Epoch: 2/20... Step: 160... Loss: 3.1630... Val Loss: 3.1232\n",
      "Epoch: 2/20... Step: 170... Loss: 3.1646... Val Loss: 3.1221\n",
      "Epoch: 2/20... Step: 180... Loss: 3.1359... Val Loss: 3.1217\n",
      "Epoch: 2/20... Step: 190... Loss: 3.1286... Val Loss: 3.1213\n",
      "Epoch: 2/20... Step: 200... Loss: 3.1626... Val Loss: 3.1206\n",
      "Epoch: 2/20... Step: 210... Loss: 3.1450... Val Loss: 3.1206\n",
      "Epoch: 2/20... Step: 220... Loss: 3.1323... Val Loss: 3.1197\n",
      "Epoch: 2/20... Step: 230... Loss: 3.1356... Val Loss: 3.1192\n",
      "Epoch: 2/20... Step: 240... Loss: 3.1553... Val Loss: 3.1189\n",
      "Epoch: 2/20... Step: 250... Loss: 3.1262... Val Loss: 3.1182\n",
      "Epoch: 2/20... Step: 260... Loss: 3.1346... Val Loss: 3.1175\n",
      "Epoch: 2/20... Step: 270... Loss: 3.1317... Val Loss: 3.1170\n",
      "Epoch: 3/20... Step: 280... Loss: 3.1242... Val Loss: 3.1162\n",
      "Epoch: 3/20... Step: 290... Loss: 3.1372... Val Loss: 3.1162\n",
      "Epoch: 3/20... Step: 300... Loss: 3.1187... Val Loss: 3.1152\n",
      "Epoch: 3/20... Step: 310... Loss: 3.1379... Val Loss: 3.1141\n",
      "Epoch: 3/20... Step: 320... Loss: 3.1444... Val Loss: 3.1131\n",
      "Epoch: 3/20... Step: 330... Loss: 3.1148... Val Loss: 3.1117\n",
      "Epoch: 3/20... Step: 340... Loss: 3.1303... Val Loss: 3.1100\n",
      "Epoch: 3/20... Step: 350... Loss: 3.1254... Val Loss: 3.1082\n",
      "Epoch: 3/20... Step: 360... Loss: 3.1190... Val Loss: 3.1057\n",
      "Epoch: 3/20... Step: 370... Loss: 3.1183... Val Loss: 3.1033\n",
      "Epoch: 3/20... Step: 380... Loss: 3.1052... Val Loss: 3.1000\n",
      "Epoch: 3/20... Step: 390... Loss: 3.1048... Val Loss: 3.0962\n",
      "Epoch: 3/20... Step: 400... Loss: 3.1091... Val Loss: 3.0916\n",
      "Epoch: 3/20... Step: 410... Loss: 3.0833... Val Loss: 3.0862\n",
      "Epoch: 4/20... Step: 420... Loss: 3.0774... Val Loss: 3.0808\n",
      "Epoch: 4/20... Step: 430... Loss: 3.0673... Val Loss: 3.0743\n",
      "Epoch: 4/20... Step: 440... Loss: 3.0743... Val Loss: 3.0657\n",
      "Epoch: 4/20... Step: 450... Loss: 3.0733... Val Loss: 3.0558\n",
      "Epoch: 4/20... Step: 460... Loss: 3.0717... Val Loss: 3.0448\n",
      "Epoch: 4/20... Step: 470... Loss: 3.0444... Val Loss: 3.0327\n",
      "Epoch: 4/20... Step: 480... Loss: 3.0583... Val Loss: 3.0211\n",
      "Epoch: 4/20... Step: 490... Loss: 3.0281... Val Loss: 3.0096\n",
      "Epoch: 4/20... Step: 500... Loss: 3.0298... Val Loss: 2.9987\n",
      "Epoch: 4/20... Step: 510... Loss: 3.0043... Val Loss: 2.9875\n",
      "Epoch: 4/20... Step: 520... Loss: 3.0192... Val Loss: 2.9767\n",
      "Epoch: 4/20... Step: 530... Loss: 2.9851... Val Loss: 2.9658\n",
      "Epoch: 4/20... Step: 540... Loss: 2.9874... Val Loss: 2.9548\n",
      "Epoch: 4/20... Step: 550... Loss: 2.9586... Val Loss: 2.9447\n",
      "Epoch: 5/20... Step: 560... Loss: 2.9737... Val Loss: 2.9362\n",
      "Epoch: 5/20... Step: 570... Loss: 2.9669... Val Loss: 2.9279\n",
      "Epoch: 5/20... Step: 580... Loss: 2.9492... Val Loss: 2.9204\n",
      "Epoch: 5/20... Step: 590... Loss: 2.9490... Val Loss: 2.9137\n",
      "Epoch: 5/20... Step: 600... Loss: 2.9551... Val Loss: 2.9076\n",
      "Epoch: 5/20... Step: 610... Loss: 2.9405... Val Loss: 2.9020\n",
      "Epoch: 5/20... Step: 620... Loss: 2.9387... Val Loss: 2.8971\n",
      "Epoch: 5/20... Step: 630... Loss: 2.9502... Val Loss: 2.8918\n",
      "Epoch: 5/20... Step: 640... Loss: 2.9445... Val Loss: 2.8870\n",
      "Epoch: 5/20... Step: 650... Loss: 2.9083... Val Loss: 2.8825\n",
      "Epoch: 5/20... Step: 660... Loss: 2.9221... Val Loss: 2.8785\n",
      "Epoch: 5/20... Step: 670... Loss: 2.8905... Val Loss: 2.8737\n",
      "Epoch: 5/20... Step: 680... Loss: 2.9049... Val Loss: 2.8698\n",
      "Epoch: 5/20... Step: 690... Loss: 2.9109... Val Loss: 2.8658\n",
      "Epoch: 6/20... Step: 700... Loss: 2.8955... Val Loss: 2.8620\n",
      "Epoch: 6/20... Step: 710... Loss: 2.8778... Val Loss: 2.8583\n",
      "Epoch: 6/20... Step: 720... Loss: 2.8944... Val Loss: 2.8542\n",
      "Epoch: 6/20... Step: 730... Loss: 2.8857... Val Loss: 2.8506\n",
      "Epoch: 6/20... Step: 740... Loss: 2.8968... Val Loss: 2.8464\n",
      "Epoch: 6/20... Step: 750... Loss: 2.8930... Val Loss: 2.8423\n",
      "Epoch: 6/20... Step: 760... Loss: 2.9126... Val Loss: 2.8376\n",
      "Epoch: 6/20... Step: 770... Loss: 2.8853... Val Loss: 2.8328\n",
      "Epoch: 6/20... Step: 780... Loss: 2.9003... Val Loss: 2.8271\n",
      "Epoch: 6/20... Step: 790... Loss: 2.8596... Val Loss: 2.8203\n",
      "Epoch: 6/20... Step: 800... Loss: 2.8650... Val Loss: 2.8132\n",
      "Epoch: 6/20... Step: 810... Loss: 2.8848... Val Loss: 2.8064\n",
      "Epoch: 6/20... Step: 820... Loss: 2.8487... Val Loss: 2.8001\n",
      "Epoch: 7/20... Step: 830... Loss: 2.8420... Val Loss: 2.7951\n",
      "Epoch: 7/20... Step: 840... Loss: 2.8530... Val Loss: 2.7878\n",
      "Epoch: 7/20... Step: 850... Loss: 2.8406... Val Loss: 2.7825\n",
      "Epoch: 7/20... Step: 860... Loss: 2.8419... Val Loss: 2.7763\n",
      "Epoch: 7/20... Step: 870... Loss: 2.8178... Val Loss: 2.7711\n",
      "Epoch: 7/20... Step: 880... Loss: 2.8074... Val Loss: 2.7652\n",
      "Epoch: 7/20... Step: 890... Loss: 2.8392... Val Loss: 2.7598\n",
      "Epoch: 7/20... Step: 900... Loss: 2.8198... Val Loss: 2.7538\n",
      "Epoch: 7/20... Step: 910... Loss: 2.8205... Val Loss: 2.7475\n",
      "Epoch: 7/20... Step: 920... Loss: 2.8226... Val Loss: 2.7423\n",
      "Epoch: 7/20... Step: 930... Loss: 2.8247... Val Loss: 2.7362\n",
      "Epoch: 7/20... Step: 940... Loss: 2.7963... Val Loss: 2.7302\n",
      "Epoch: 7/20... Step: 950... Loss: 2.7991... Val Loss: 2.7262\n",
      "Epoch: 7/20... Step: 960... Loss: 2.7991... Val Loss: 2.7182\n",
      "Epoch: 8/20... Step: 970... Loss: 2.7838... Val Loss: 2.7112\n",
      "Epoch: 8/20... Step: 980... Loss: 2.8036... Val Loss: 2.7044\n",
      "Epoch: 8/20... Step: 990... Loss: 2.7721... Val Loss: 2.6968\n",
      "Epoch: 8/20... Step: 1000... Loss: 2.7877... Val Loss: 2.6907\n",
      "Epoch: 8/20... Step: 1010... Loss: 2.7898... Val Loss: 2.6845\n",
      "Epoch: 8/20... Step: 1020... Loss: 2.7667... Val Loss: 2.6765\n",
      "Epoch: 8/20... Step: 1030... Loss: 2.7685... Val Loss: 2.6692\n",
      "Epoch: 8/20... Step: 1040... Loss: 2.7568... Val Loss: 2.6624\n",
      "Epoch: 8/20... Step: 1050... Loss: 2.7485... Val Loss: 2.6562\n",
      "Epoch: 8/20... Step: 1060... Loss: 2.7446... Val Loss: 2.6505\n",
      "Epoch: 8/20... Step: 1070... Loss: 2.7396... Val Loss: 2.6452\n",
      "Epoch: 8/20... Step: 1080... Loss: 2.7269... Val Loss: 2.6377\n",
      "Epoch: 8/20... Step: 1090... Loss: 2.7302... Val Loss: 2.6325\n",
      "Epoch: 8/20... Step: 1100... Loss: 2.7163... Val Loss: 2.6270\n",
      "Epoch: 9/20... Step: 1110... Loss: 2.7004... Val Loss: 2.6224\n",
      "Epoch: 9/20... Step: 1120... Loss: 2.6877... Val Loss: 2.6176\n",
      "Epoch: 9/20... Step: 1130... Loss: 2.7042... Val Loss: 2.6112\n",
      "Epoch: 9/20... Step: 1140... Loss: 2.7080... Val Loss: 2.6055\n",
      "Epoch: 9/20... Step: 1150... Loss: 2.7087... Val Loss: 2.6015\n",
      "Epoch: 9/20... Step: 1160... Loss: 2.6823... Val Loss: 2.5957\n",
      "Epoch: 9/20... Step: 1170... Loss: 2.7079... Val Loss: 2.5913\n",
      "Epoch: 9/20... Step: 1180... Loss: 2.6796... Val Loss: 2.5872\n",
      "Epoch: 9/20... Step: 1190... Loss: 2.6912... Val Loss: 2.5818\n",
      "Epoch: 9/20... Step: 1200... Loss: 2.6707... Val Loss: 2.5783\n",
      "Epoch: 9/20... Step: 1210... Loss: 2.6961... Val Loss: 2.5728\n",
      "Epoch: 9/20... Step: 1220... Loss: 2.6567... Val Loss: 2.5675\n",
      "Epoch: 9/20... Step: 1230... Loss: 2.6640... Val Loss: 2.5630\n",
      "Epoch: 9/20... Step: 1240... Loss: 2.6536... Val Loss: 2.5602\n",
      "Epoch: 10/20... Step: 1250... Loss: 2.6614... Val Loss: 2.5554\n",
      "Epoch: 10/20... Step: 1260... Loss: 2.6572... Val Loss: 2.5507\n",
      "Epoch: 10/20... Step: 1270... Loss: 2.6496... Val Loss: 2.5462\n",
      "Epoch: 10/20... Step: 1280... Loss: 2.6441... Val Loss: 2.5429\n",
      "Epoch: 10/20... Step: 1290... Loss: 2.6467... Val Loss: 2.5383\n",
      "Epoch: 10/20... Step: 1300... Loss: 2.6397... Val Loss: 2.5363\n",
      "Epoch: 10/20... Step: 1310... Loss: 2.6420... Val Loss: 2.5329\n",
      "Epoch: 10/20... Step: 1320... Loss: 2.6482... Val Loss: 2.5277\n",
      "Epoch: 10/20... Step: 1330... Loss: 2.6548... Val Loss: 2.5248\n",
      "Epoch: 10/20... Step: 1340... Loss: 2.6176... Val Loss: 2.5208\n",
      "Epoch: 10/20... Step: 1350... Loss: 2.6201... Val Loss: 2.5167\n",
      "Epoch: 10/20... Step: 1360... Loss: 2.5977... Val Loss: 2.5127\n",
      "Epoch: 10/20... Step: 1370... Loss: 2.6145... Val Loss: 2.5100\n",
      "Epoch: 10/20... Step: 1380... Loss: 2.6247... Val Loss: 2.5057\n",
      "Epoch: 11/20... Step: 1390... Loss: 2.6031... Val Loss: 2.5019\n",
      "Epoch: 11/20... Step: 1400... Loss: 2.5810... Val Loss: 2.4994\n",
      "Epoch: 11/20... Step: 1410... Loss: 2.5946... Val Loss: 2.4963\n",
      "Epoch: 11/20... Step: 1420... Loss: 2.5972... Val Loss: 2.4934\n",
      "Epoch: 11/20... Step: 1430... Loss: 2.6157... Val Loss: 2.4894\n",
      "Epoch: 11/20... Step: 1440... Loss: 2.6035... Val Loss: 2.4862\n",
      "Epoch: 11/20... Step: 1450... Loss: 2.6143... Val Loss: 2.4831\n",
      "Epoch: 11/20... Step: 1460... Loss: 2.5883... Val Loss: 2.4797\n",
      "Epoch: 11/20... Step: 1470... Loss: 2.6147... Val Loss: 2.4771\n",
      "Epoch: 11/20... Step: 1480... Loss: 2.5811... Val Loss: 2.4731\n",
      "Epoch: 11/20... Step: 1490... Loss: 2.5927... Val Loss: 2.4699\n",
      "Epoch: 11/20... Step: 1500... Loss: 2.5946... Val Loss: 2.4662\n",
      "Epoch: 11/20... Step: 1510... Loss: 2.5707... Val Loss: 2.4641\n",
      "Epoch: 12/20... Step: 1520... Loss: 2.5564... Val Loss: 2.4604\n",
      "Epoch: 12/20... Step: 1530... Loss: 2.5732... Val Loss: 2.4581\n",
      "Epoch: 12/20... Step: 1540... Loss: 2.5692... Val Loss: 2.4547\n",
      "Epoch: 12/20... Step: 1550... Loss: 2.5600... Val Loss: 2.4520\n",
      "Epoch: 12/20... Step: 1560... Loss: 2.5467... Val Loss: 2.4495\n",
      "Epoch: 12/20... Step: 1570... Loss: 2.5560... Val Loss: 2.4474\n",
      "Epoch: 12/20... Step: 1580... Loss: 2.5659... Val Loss: 2.4435\n",
      "Epoch: 12/20... Step: 1590... Loss: 2.5564... Val Loss: 2.4413\n",
      "Epoch: 12/20... Step: 1600... Loss: 2.5592... Val Loss: 2.4399\n",
      "Epoch: 12/20... Step: 1610... Loss: 2.5589... Val Loss: 2.4362\n",
      "Epoch: 12/20... Step: 1620... Loss: 2.5517... Val Loss: 2.4328\n",
      "Epoch: 12/20... Step: 1630... Loss: 2.5348... Val Loss: 2.4312\n",
      "Epoch: 12/20... Step: 1640... Loss: 2.5418... Val Loss: 2.4278\n",
      "Epoch: 12/20... Step: 1650... Loss: 2.5491... Val Loss: 2.4258\n",
      "Epoch: 13/20... Step: 1660... Loss: 2.5326... Val Loss: 2.4234\n",
      "Epoch: 13/20... Step: 1670... Loss: 2.5580... Val Loss: 2.4221\n",
      "Epoch: 13/20... Step: 1680... Loss: 2.5309... Val Loss: 2.4180\n",
      "Epoch: 13/20... Step: 1690... Loss: 2.5546... Val Loss: 2.4170\n",
      "Epoch: 13/20... Step: 1700... Loss: 2.5572... Val Loss: 2.4145\n",
      "Epoch: 13/20... Step: 1710... Loss: 2.5294... Val Loss: 2.4117\n",
      "Epoch: 13/20... Step: 1720... Loss: 2.5340... Val Loss: 2.4105\n",
      "Epoch: 13/20... Step: 1730... Loss: 2.5461... Val Loss: 2.4091\n",
      "Epoch: 13/20... Step: 1740... Loss: 2.5301... Val Loss: 2.4060\n",
      "Epoch: 13/20... Step: 1750... Loss: 2.5024... Val Loss: 2.4042\n",
      "Epoch: 13/20... Step: 1760... Loss: 2.5248... Val Loss: 2.4019\n",
      "Epoch: 13/20... Step: 1770... Loss: 2.5162... Val Loss: 2.3998\n",
      "Epoch: 13/20... Step: 1780... Loss: 2.5123... Val Loss: 2.3980\n",
      "Epoch: 13/20... Step: 1790... Loss: 2.5105... Val Loss: 2.3956\n",
      "Epoch: 14/20... Step: 1800... Loss: 2.5036... Val Loss: 2.3939\n",
      "Epoch: 14/20... Step: 1810... Loss: 2.4864... Val Loss: 2.3927\n",
      "Epoch: 14/20... Step: 1820... Loss: 2.5061... Val Loss: 2.3900\n",
      "Epoch: 14/20... Step: 1830... Loss: 2.5174... Val Loss: 2.3896\n",
      "Epoch: 14/20... Step: 1840... Loss: 2.5159... Val Loss: 2.3867\n",
      "Epoch: 14/20... Step: 1850... Loss: 2.4843... Val Loss: 2.3855\n",
      "Epoch: 14/20... Step: 1860... Loss: 2.5129... Val Loss: 2.3848\n",
      "Epoch: 14/20... Step: 1870... Loss: 2.4933... Val Loss: 2.3826\n",
      "Epoch: 14/20... Step: 1880... Loss: 2.5084... Val Loss: 2.3811\n",
      "Epoch: 14/20... Step: 1890... Loss: 2.4806... Val Loss: 2.3796\n",
      "Epoch: 14/20... Step: 1900... Loss: 2.5246... Val Loss: 2.3771\n",
      "Epoch: 14/20... Step: 1910... Loss: 2.4677... Val Loss: 2.3748\n",
      "Epoch: 14/20... Step: 1920... Loss: 2.4915... Val Loss: 2.3737\n",
      "Epoch: 14/20... Step: 1930... Loss: 2.4878... Val Loss: 2.3717\n",
      "Epoch: 15/20... Step: 1940... Loss: 2.4934... Val Loss: 2.3698\n",
      "Epoch: 15/20... Step: 1950... Loss: 2.4885... Val Loss: 2.3678\n",
      "Epoch: 15/20... Step: 1960... Loss: 2.4840... Val Loss: 2.3671\n",
      "Epoch: 15/20... Step: 1970... Loss: 2.4809... Val Loss: 2.3658\n",
      "Epoch: 15/20... Step: 1980... Loss: 2.4806... Val Loss: 2.3636\n",
      "Epoch: 15/20... Step: 1990... Loss: 2.4853... Val Loss: 2.3622\n",
      "Epoch: 15/20... Step: 2000... Loss: 2.4938... Val Loss: 2.3614\n",
      "Epoch: 15/20... Step: 2010... Loss: 2.4847... Val Loss: 2.3604\n",
      "Epoch: 15/20... Step: 2020... Loss: 2.5077... Val Loss: 2.3581\n",
      "Epoch: 15/20... Step: 2030... Loss: 2.4671... Val Loss: 2.3563\n",
      "Epoch: 15/20... Step: 2040... Loss: 2.4604... Val Loss: 2.3555\n",
      "Epoch: 15/20... Step: 2050... Loss: 2.4468... Val Loss: 2.3538\n",
      "Epoch: 15/20... Step: 2060... Loss: 2.4676... Val Loss: 2.3524\n",
      "Epoch: 15/20... Step: 2070... Loss: 2.4857... Val Loss: 2.3509\n",
      "Epoch: 16/20... Step: 2080... Loss: 2.4620... Val Loss: 2.3499\n",
      "Epoch: 16/20... Step: 2090... Loss: 2.4375... Val Loss: 2.3479\n",
      "Epoch: 16/20... Step: 2100... Loss: 2.4558... Val Loss: 2.3471\n",
      "Epoch: 16/20... Step: 2110... Loss: 2.4595... Val Loss: 2.3461\n",
      "Epoch: 16/20... Step: 2120... Loss: 2.4875... Val Loss: 2.3451\n",
      "Epoch: 16/20... Step: 2130... Loss: 2.4665... Val Loss: 2.3435\n",
      "Epoch: 16/20... Step: 2140... Loss: 2.4814... Val Loss: 2.3422\n",
      "Epoch: 16/20... Step: 2150... Loss: 2.4595... Val Loss: 2.3415\n",
      "Epoch: 16/20... Step: 2160... Loss: 2.4951... Val Loss: 2.3392\n",
      "Epoch: 16/20... Step: 2170... Loss: 2.4480... Val Loss: 2.3376\n",
      "Epoch: 16/20... Step: 2180... Loss: 2.4454... Val Loss: 2.3369\n",
      "Epoch: 16/20... Step: 2190... Loss: 2.4721... Val Loss: 2.3351\n",
      "Epoch: 16/20... Step: 2200... Loss: 2.4420... Val Loss: 2.3342\n",
      "Epoch: 17/20... Step: 2210... Loss: 2.4394... Val Loss: 2.3329\n",
      "Epoch: 17/20... Step: 2220... Loss: 2.4519... Val Loss: 2.3319\n",
      "Epoch: 17/20... Step: 2230... Loss: 2.4444... Val Loss: 2.3305\n",
      "Epoch: 17/20... Step: 2240... Loss: 2.4469... Val Loss: 2.3303\n",
      "Epoch: 17/20... Step: 2250... Loss: 2.4276... Val Loss: 2.3289\n",
      "Epoch: 17/20... Step: 2260... Loss: 2.4322... Val Loss: 2.3267\n",
      "Epoch: 17/20... Step: 2270... Loss: 2.4449... Val Loss: 2.3258\n",
      "Epoch: 17/20... Step: 2280... Loss: 2.4349... Val Loss: 2.3250\n",
      "Epoch: 17/20... Step: 2290... Loss: 2.4390... Val Loss: 2.3239\n",
      "Epoch: 17/20... Step: 2300... Loss: 2.4541... Val Loss: 2.3224\n",
      "Epoch: 17/20... Step: 2310... Loss: 2.4467... Val Loss: 2.3210\n",
      "Epoch: 17/20... Step: 2320... Loss: 2.4268... Val Loss: 2.3207\n",
      "Epoch: 17/20... Step: 2330... Loss: 2.4371... Val Loss: 2.3202\n",
      "Epoch: 17/20... Step: 2340... Loss: 2.4377... Val Loss: 2.3183\n",
      "Epoch: 18/20... Step: 2350... Loss: 2.4212... Val Loss: 2.3169\n",
      "Epoch: 18/20... Step: 2360... Loss: 2.4468... Val Loss: 2.3160\n",
      "Epoch: 18/20... Step: 2370... Loss: 2.4255... Val Loss: 2.3146\n",
      "Epoch: 18/20... Step: 2380... Loss: 2.4539... Val Loss: 2.3143\n",
      "Epoch: 18/20... Step: 2390... Loss: 2.4524... Val Loss: 2.3134\n",
      "Epoch: 18/20... Step: 2400... Loss: 2.4315... Val Loss: 2.3122\n",
      "Epoch: 18/20... Step: 2410... Loss: 2.4362... Val Loss: 2.3111\n",
      "Epoch: 18/20... Step: 2420... Loss: 2.4511... Val Loss: 2.3102\n",
      "Epoch: 18/20... Step: 2430... Loss: 2.4355... Val Loss: 2.3093\n",
      "Epoch: 18/20... Step: 2440... Loss: 2.4199... Val Loss: 2.3077\n",
      "Epoch: 18/20... Step: 2450... Loss: 2.4306... Val Loss: 2.3061\n",
      "Epoch: 18/20... Step: 2460... Loss: 2.4255... Val Loss: 2.3049\n",
      "Epoch: 18/20... Step: 2470... Loss: 2.4238... Val Loss: 2.3046\n",
      "Epoch: 18/20... Step: 2480... Loss: 2.4100... Val Loss: 2.3040\n",
      "Epoch: 19/20... Step: 2490... Loss: 2.4080... Val Loss: 2.3025\n",
      "Epoch: 19/20... Step: 2500... Loss: 2.4078... Val Loss: 2.3019\n",
      "Epoch: 19/20... Step: 2510... Loss: 2.4200... Val Loss: 2.3005\n",
      "Epoch: 19/20... Step: 2520... Loss: 2.4231... Val Loss: 2.3005\n",
      "Epoch: 19/20... Step: 2530... Loss: 2.4222... Val Loss: 2.2992\n",
      "Epoch: 19/20... Step: 2540... Loss: 2.3966... Val Loss: 2.2986\n",
      "Epoch: 19/20... Step: 2550... Loss: 2.4271... Val Loss: 2.2976\n",
      "Epoch: 19/20... Step: 2560... Loss: 2.4138... Val Loss: 2.2970\n",
      "Epoch: 19/20... Step: 2570... Loss: 2.4160... Val Loss: 2.2966\n",
      "Epoch: 19/20... Step: 2580... Loss: 2.4050... Val Loss: 2.2943\n",
      "Epoch: 19/20... Step: 2590... Loss: 2.4499... Val Loss: 2.2933\n",
      "Epoch: 19/20... Step: 2600... Loss: 2.3876... Val Loss: 2.2921\n",
      "Epoch: 19/20... Step: 2610... Loss: 2.4092... Val Loss: 2.2918\n",
      "Epoch: 19/20... Step: 2620... Loss: 2.3945... Val Loss: 2.2902\n",
      "Epoch: 20/20... Step: 2630... Loss: 2.4118... Val Loss: 2.2892\n",
      "Epoch: 20/20... Step: 2640... Loss: 2.3918... Val Loss: 2.2884\n",
      "Epoch: 20/20... Step: 2650... Loss: 2.4150... Val Loss: 2.2876\n",
      "Epoch: 20/20... Step: 2660... Loss: 2.4028... Val Loss: 2.2877\n",
      "Epoch: 20/20... Step: 2670... Loss: 2.4257... Val Loss: 2.2864\n",
      "Epoch: 20/20... Step: 2680... Loss: 2.3993... Val Loss: 2.2855\n",
      "Epoch: 20/20... Step: 2690... Loss: 2.4116... Val Loss: 2.2847\n",
      "Epoch: 20/20... Step: 2700... Loss: 2.4129... Val Loss: 2.2843\n",
      "Epoch: 20/20... Step: 2710... Loss: 2.4238... Val Loss: 2.2828\n",
      "Epoch: 20/20... Step: 2720... Loss: 2.3983... Val Loss: 2.2817\n",
      "Epoch: 20/20... Step: 2730... Loss: 2.3915... Val Loss: 2.2806\n",
      "Epoch: 20/20... Step: 2740... Loss: 2.3709... Val Loss: 2.2798\n",
      "Epoch: 20/20... Step: 2750... Loss: 2.4002... Val Loss: 2.2796\n",
      "Epoch: 20/20... Step: 2760... Loss: 2.4192... Val Loss: 2.2784\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfZxvNoDceEm"
   },
   "source": [
    "## 4. Применение модели\n",
    "\n",
    "Сперва сохраним обученную модель, чтобы можно было загрузить её позже. В следующей ячейке сохраняются параметры, необходимые для создания той же архитектуры, гиперпараметры скрытого слоя и токены."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q6RXl5VAceEm"
   },
   "source": [
    "checkpoint = {\"n_hidden\": net.n_hidden,\n",
    "              \"n_layers\": net.n_layers,\n",
    "              \"state_dict\": net.state_dict(),\n",
    "              \"tokens\": net.unique_tokens}\n",
    "\n",
    "with open(\"rnn_x_epoch.net\", \"wb\") as f:\n",
    "    torch.save(checkpoint, f)"
   ],
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2sJhx5iceEm"
   },
   "source": [
    "### Делаем предсказания\n",
    "\n",
    "Сгенерируем текст! Для предсказания продолжения текста мы передаём в сеть последний символ, она предсказывает следующий символ, который мы снова передаем на вход, получаем ещё один предсказанный символ и так далее. Наши прогнозы основаны на категориальном распределении вероятностей по всем возможным символам. Мы можем ограничить число символов на каждом шаге генерации, чтобы сделать получаемый предсказанный текст более разумным, рассматривая только некоторые, наиболее вероятные символы. С одной стороны, такой подход позволит нам рассматривать не только самую вероятную последовательность с точки зрения прогноза модели. С другой стороны, мы будем работать с ограниченным набором сгенерированных вариантов, поэтому избавимся от совсем уж шумовых прогнозов. Узнать больше можно [здесь](https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QEIRW_B2ceEm"
   },
   "source": [
    "def predict_next_char(net: torch.nn.Module, char: str, h: torch.Tensor = None, top_k: int = None) -> Tuple[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Given a character and a model, predicts next character in the sequence\n",
    "        :param net:\n",
    "        :param char:\n",
    "        :param h:\n",
    "        :param top_k:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.unique_tokens))\n",
    "        inputs = torch.from_numpy(x).to(device)\n",
    "\n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data.cpu()\n",
    "\n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.unique_tokens))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ],
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OG38j3gQceEm"
   },
   "source": [
    "### Priming и генерирование текста\n",
    "\n",
    "Нужно задать скрытое состояние, чтобы сеть не генерировала произвольные символы. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P9vpB5gRceEm"
   },
   "source": [
    "def sample(net, size, prime=\"The\", top_k=None):\n",
    "    net.to(device)\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1, device)\n",
    "    for ch in prime:\n",
    "        char, h = predict_next_char(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict_next_char(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return \"\".join(chars)"
   ],
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "942mjdQHceEm"
   },
   "source": [
    "### Загрузка чекпоинта и генерация"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Xt9ldUuSceEm"
   },
   "source": [
    "with open(\"rnn_x_epoch.net\", \"rb\") as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint[\"tokens\"], n_hidden=checkpoint[\"n_hidden\"], n_layers=checkpoint[\"n_layers\"])\n",
    "loaded.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
   ],
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said hes had tardes he adder asten ang ther tan has that sinter sare the to seed tantare to the ther tould she wons,\n",
      "titite thad ans,\n",
      "an to to than whout he than to soot\n",
      "had shas her. In sher ting\n",
      "he adt the att hares\n",
      "and woul hit ase,\n",
      "thit ottel houd, to, his tisilind the arset the, house hes, har ad ard thot at adsisthint hor thound, hir salt,,\" shid,, wesins he wos ot\n",
      "hore at to woust the sale the hesens othes wend whas wees the\n",
      "hertes to she\n",
      "she her seen of,, wout thy at ho the sat the hose wont we hor ant to a he thoet the wist. Is the, tho was wat wile teer heed the sertas sore so as tangad the telt, white an shing sore ther horsad at sorend hins one hes whonilt ad ass hir thint, wan so the tored\n",
      "was, she anget he sisten shit to\n",
      "an of so the wat sat adsind so her tho he\n",
      "hand her hat anse so soos thit,\n",
      "her and, hot had tand,\n",
      "the, was, an ate sast tart she to tas shad, sit the adlint ang at, he ant at\n",
      "hat tas tine shose hanse he wese hon as sore to so as this sittitt so ard thas sher here hare that thint shoras that he\n",
      "woung tho her of sititade hit,\n",
      "he than an of huter the seer to\n",
      "andits os\n",
      "heen, to wors an sas adtater that war harsens the thas sitansen an ontet ho serat at. \"It\n",
      "tars the tore he thore than an the seed she the was war the hes to whon soes her the wored.\" Tals\n",
      "war so of hit the he ard the he not hit shers the the tat, whet anghing\n",
      "teits a hat, herat he thit he than, hid hes her\n",
      "he ad sith ads sellen that hin one as to are the wat he hes san as asd,\" the tors of houd ot a wet of and, honten he her thing hit her, as the woth ot og, horte the asthes. \"Hit\n",
      "ard of te ond hin thor tint soes shid the\n",
      "atd one the tortad shid and tantataned shat a thit thad asd\n",
      "ars hing tas of to sorid thirs tint at\n",
      "atend ans ond the\n",
      "wond wat ant ot tare wal sousd so serted thit the\n",
      "ard sint tho he hen sassithy ant the\n",
      "tas han her ant sors os he shind anging hansens teins\n",
      "thas hin teind that wotind as to ad arled the, ang os\n",
      "the and sas hee siride and. \"Is soust, ting her. As s\n"
     ]
    }
   ]
  }
 ]
}
