{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "colab": {
   "name": "sem08_solution.ipynb",
   "provenance": [],
   "collapsed_sections": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1W8R8WgZceEk"
   },
   "source": [
    "# Семинар 9: Character-Level LSTM\n",
    "\n",
    "## Вступление\n",
    "На прошлом занятии мы познакомились с тем, как можно векторизовать текстовые данные для решения задач обработки текстов. Сегодня мы продолжим заниматься текстами и посмотрим на простейший пример автоматической генерации текстов при помощи Recurrent Neural Network (RNN).\n",
    "\n",
    "Полезные материалы по RNN можно почитать [здесь]((http://karpathy.github.io/2015/05/21/rnn-effectiveness/)), а реализацию на PyTorch — [здесь](https://github.com/karpathy/char-rnn).\n",
    "\n",
    "### План семинара\n",
    "1. Подготовка данных\n",
    "2. Имплементация модели\n",
    "3. Обучение модели\n",
    "4. Применение модели"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "sqUOE2flceEl"
   },
   "source": [
    "from typing import Iterable, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_wHfCDyzceEl"
   },
   "source": [
    "## 1. Подготовка данных\n",
    "\n",
    "### Загрузим текст \"Анны Карениной\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b34kfqIOceEl"
   },
   "source": [
    "with open(\"anna.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "text[:100]"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iC21bopceEl"
   },
   "source": [
    "### Токенизируем текст\n",
    "\n",
    "Аналогично предыдущему семинару, в ячейках ниже создадим два **словаря** для преобразования символов в целые числа и обратно."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tYVlmnxLceEl"
   },
   "source": [
    "unique_chars = tuple(set(text))\n",
    "int2char = dict(enumerate(unique_chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = torch.tensor([char2int[ch] for ch in text])\n",
    "encoded[:100]"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([72, 60, 22, 33, 74, 51, 53,  0, 11,  3,  3,  3, 50, 22, 33, 33, 58,  0,\n        41, 22,  8, 64, 34, 64, 51, 21,  0, 22, 53, 51,  0, 22, 34, 34,  0, 22,\n        34, 64, 67, 51, 31,  0, 51, 61, 51, 53, 58,  0, 80, 77, 60, 22, 33, 33,\n        58,  0, 41, 22,  8, 64, 34, 58,  0, 64, 21,  0, 80, 77, 60, 22, 33, 33,\n        58,  0, 64, 77,  0, 64, 74, 21,  0, 54, 29, 77,  3, 29, 22, 58, 40,  3,\n         3, 79, 61, 51, 53, 58, 74, 60, 64, 77])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azltQy-gceEl"
   },
   "source": [
    "Посмотрим на схему char-RNN:\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charseq.jpeg?raw=1\" width=\"30%\">\n",
    "\n",
    "Сеть ожидает **one-hot encoded** входа, что означает, что каждый символ преобразуется в целое число (через созданный маппинг), а затем преобразуется в вектор-столбец, где только соответствующий ему целочисленный индекс будет иметь значение 1, а остальная часть вектора будет заполнена нулями. Давайте напишем функцию для этого преобразования.\n",
    "\n",
    "#### Задание: допишите функцию one-hot кодирования последовательности"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OnahALhiceEl"
   },
   "source": [
    "def one_hot_encode(int_words: torch.Tensor, n_labels: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates one-hot representation matrix for a given batch of integer sequences\n",
    "    :param int_words: tensor of ints, which represents current sequence; shape: [batch_size, seq_len]\n",
    "    :param n_labels: vocabulary size (number of unique tokens in data)\n",
    "    :return: one-hot representation of the input tensor; shape: [batch_size, seq_len, n_labels]\n",
    "    \"\"\"\n",
    "    words_one_hot = torch.zeros((int_words.numel(), n_labels), dtype=torch.float32)\n",
    "    words_one_hot[torch.arange(words_one_hot.shape[0]), int_words.flatten()] = 1.\n",
    "    words_one_hot = words_one_hot.reshape((*int_words.shape, n_labels))\n",
    "\n",
    "    return words_one_hot"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# testing the function\n",
    "test_seq = torch.tensor([[3, 5, 1], [0, 2, 4]])\n",
    "test_one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "print(test_one_hot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YyL91CuceEl"
   },
   "source": [
    "### Сформируем батчи\n",
    "На простом примере батчи будут выглядеть так: мы возьмем закодированные символы и разделим их на несколько последовательностей, заданных параметром `batch_size`. Каждая из наших последовательностей будет иметь длину `seq_length`.\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/sequence_batching@1x.png?raw=1\" width=500px>\n",
    "\n",
    "**1. Отбросим часть текста, чтобы у нас были только полные батчи**\n",
    "\n",
    "Каждый батч содержит $N \\times M$ символов, где $N$ — это количество последовательностей в батче (`batch_size`), а $M$ — длина каждой последовательности (`seq_length`). Затем, чтобы получить общее количество батчей $K$, которое мы можем сделать из последовательности, нужно разделить длину последовательности на количество символов в батче. Когда мы узнаем количество батчей, можно получить общее количество символов, которые нужно сохранить, из последовательности: $N \\times M \\times K$.\n",
    "\n",
    "**2. Разделим текст на $N$ частей**\n",
    "\n",
    "Этот шаг нужен, чтобы мы могли проходить по тексту окном размера `[batch_size, seq_len]`. Его можно реализовать при помощи простого `reshape`.\n",
    "\n",
    "**3. Теперь, когда у нас готова матрица текста, мы можем двигаться по ней окном, чтобы получить батчи**\n",
    "\n",
    "Из каждой позиции окна сформируем обучающие пары `(x, y)` следующим образом: $x$ — это все элементы окна кроме последнего столбца, а $y$ — это все элементы окна кроме первого столбца. Тем самым для каждого токена исходного текста мы будем предсказывать следующий за ним токен.\n",
    "\n",
    "#### Задание: допишите функцию генерации батчей"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2ECftYejnvpx"
   },
   "source": [
    "def get_batches(int_words: torch.Tensor, batch_size: int, seq_length: int) -> Iterable[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Generates batches from encoded sequence.\n",
    "    :param int_words: array of ints, which represents the text; shape: [batch_size, -1]\n",
    "    :param batch_size: number of sequences per batch\n",
    "    :param seq_length: number of encoded chars in a sequence\n",
    "    :return: generator of pairs (x, y); x_shape, y_shape: [batch_size, seq_length - 1]\n",
    "    \"\"\"\n",
    "    # 1. Truncate text, so there are only full batches\n",
    "    window_size = seq_length + 1\n",
    "    batch_size_total = batch_size * window_size\n",
    "    n_batches = len(int_words) // batch_size_total\n",
    "    int_words = int_words[:n_batches * batch_size_total]\n",
    "\n",
    "    # 2. Reshape into batch_size rows\n",
    "    int_words = int_words.reshape((batch_size, -1))\n",
    "\n",
    "    # 3. Iterate through the text matrix\n",
    "    for position in range(0, int_words.shape[1], window_size):\n",
    "        x = int_words[:, position:position + window_size - 1]\n",
    "        y = int_words[:, position + 1:position + window_size]\n",
    "        yield x, y"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qtKlLXi1ceEl"
   },
   "source": [
    "# testing the function\n",
    "test_batches = get_batches(encoded, 8, 50)\n",
    "test_x, test_y = next(test_batches)\n",
    "assert test_x.shape == test_y.shape\n",
    "print(f\"x:\\n{test_x[:10, :10]}\\n\")\n",
    "print(f\"y:\\n{test_y[:10, :10]}\")"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      "tensor([[72, 60, 22, 33, 74, 51, 53,  0, 11,  3],\n",
      "        [33, 33, 51,  7,  0, 22, 77,  7,  0, 21],\n",
      "        [74, 60, 22, 74,  0, 64, 77, 21, 54, 34],\n",
      "        [51, 29,  0, 74, 60, 54, 80, 17, 60, 74],\n",
      "        [51, 53, 70,  0, 60, 51,  0, 60, 22,  7],\n",
      "        [53, 70,  0, 29, 60, 54,  0, 29, 22, 21],\n",
      "        [21, 74,  0, 16, 51,  0, 57, 54, 61, 51],\n",
      "        [70,  0, 45, 34, 51, 20, 51, 58,  0, 45]])\n",
      "\n",
      "y:\n",
      "tensor([[60, 22, 33, 74, 51, 53,  0, 11,  3,  3],\n",
      "        [33, 51,  7,  0, 22, 77,  7,  0, 21, 60],\n",
      "        [60, 22, 74,  0, 64, 77, 21, 54, 34, 80],\n",
      "        [29,  0, 74, 60, 54, 80, 17, 60, 74, 41],\n",
      "        [53, 70,  0, 60, 51,  0, 60, 22,  7,  0],\n",
      "        [70,  0, 29, 60, 54,  0, 29, 22, 21,  0],\n",
      "        [74,  0, 16, 51,  0, 57, 54, 61, 51, 53],\n",
      "        [ 0, 45, 34, 51, 20, 51, 58,  0, 45, 34]])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jouxv0L2ceEl"
   },
   "source": [
    "## 2. Имплементация модели\n",
    "\n",
    "<img src=\"https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/assets/charRNN.png?raw=1\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E7s5eRaoceEl"
   },
   "source": [
    "### Структура модели\n",
    "\n",
    "* Создаём и храним необходимые словари.\n",
    "* Определяем слой [LSTM]((https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM)) с помощью инстанса класса `torch.nn.LSTM`, который принимает набор параметров: `input_size` — длина последовательности в батче; `n_hidden` — размер скрытых слоёв; `n_layers` — количество слоёв; `drop_prob` — вероятность дропаута; и `batch_first` — флаг, указывающий на то, что у входных последовательностей размерность батча идёт вдоль нулевой оси.\n",
    "* Определяем слой Dropout с таким же значением `drop_prob`.\n",
    "* Определяем полносвязный слой с набором параметров: размерность ввода — `n_hidden`; размерность выхода — размер словаря.\n",
    "* Наконец, инициализируем веса и начальное скрытое состояние (`self.init_hidden()`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VPq1EA38rBqn"
   },
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        unique_tokens: Tuple[str],\n",
    "        n_hidden: int = 256,\n",
    "        n_layers: int = 2,\n",
    "        drop_prob: float = 0.5,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "        # create mappings\n",
    "        self.unique_tokens = unique_tokens\n",
    "        self.int2char = dict(enumerate(self.unique_tokens))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## define the LSTM, dropout and fully connected layers\n",
    "        self.lstm = nn.LSTM(len(self.unique_tokens), n_hidden, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.unique_tokens))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, hidden: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        # Stack up LSTM outputs using view. You may need to use contiguous to reshape the output.\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        ## Get the output for classification.\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size: int, weight_device: torch.device) -> Tuple[torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Creates two new zero tensors for hidden state and cell state of LSTM\n",
    "        :param batch_size: number of sequences per batch\n",
    "        :param weight_device: torch.device(\"cuda\") for GPU init or torch.device(\"cpu\") for CPU init\n",
    "        :return: tuple of two tensors of shape [n_layers x batch_size x n_hidden]\n",
    "        \"\"\"\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(weight_device),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(weight_device))\n",
    "        return hidden"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5IrBRlEPceEl"
   },
   "source": [
    "## 3. Обучение модели\n",
    "\n",
    "По классике, используем оптимизатор Adam и кросс-энтропию. Но без пары особенностей не обойтись:\n",
    "* Во время цикла будем отделять скрытое состояние от его истории, потому что скрытое состояние LSTM является кортежем скрытых состояний.\n",
    "* Будем использовать gradient clipping, чтобы избавиться от взрывающихся градиентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class AnnaData(torch.utils.data.IterableDataset):\n",
    "    def __init__(self, int_words: torch.Tensor, batch_size: int, seq_length: int):\n",
    "        self.int_words = int_words\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __iter__(self):\n",
    "        return get_batches(self.int_words, self.batch_size, self.seq_length)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "class CharRNNModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            unique_tokens: Tuple[str],\n",
    "            n_hidden: int = 1024,\n",
    "            n_layers: int = 2,\n",
    "            drop_prob: float = 0.5,\n",
    "            batch_size: int = 128,\n",
    "            seq_length = 256,\n",
    "            lr: float = 0.001\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model = CharRNN(unique_tokens, n_hidden, n_layers, drop_prob)\n",
    "        self.hidden = self.model.init_hidden(batch_size, self.device)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        self.n_chars = len(unique_tokens)\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def training_step(self, train_batch):\n",
    "        x, y = train_batch\n",
    "        x, y = x.squeeze(0), y.squeeze(0)\n",
    "        x = one_hot_encode(x, self.n_chars)\n",
    "\n",
    "        h = tuple([each.data for each in self.hidden])\n",
    "        output, h = self.model(x, h)\n",
    "        loss = self.loss(output, y.reshape(self.batch_size * self.seq_length).long())\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type             | Params\n",
      "-------------------------------------------\n",
      "0 | model | CharRNN          | 26.2 K\n",
      "1 | loss  | CrossEntropyLoss | 0     \n",
      "-------------------------------------------\n",
      "26.2 K    Trainable params\n",
      "0         Non-trainable params\n",
      "26.2 K    Total params\n",
      "0.105     Total estimated model params size (MB)\n",
      "/usr/local/anaconda3/envs/ml/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:236: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "188e7d67ed9343deb98234bddc2e678d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=15` reached.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = AnnaData(encoded, batch_size=128, seq_length=256)\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,  # batching is already implemented on our side\n",
    "    shuffle=False,\n",
    ")\n",
    "char_rnn = CharRNNModule(unique_chars, n_hidden=32)\n",
    "trainer = pl.Trainer(accelerator=\"cpu\", max_epochs=15)\n",
    "trainer.fit(char_rnn, train_dataloaders=train_dataloader)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZfZxvNoDceEm"
   },
   "source": [
    "## 4. Применение модели\n",
    "\n",
    "Сперва сохраним обученную модель, чтобы можно было загрузить её позже. В следующей ячейке сохраняются параметры, необходимые для создания той же архитектуры, гиперпараметры скрытого слоя и токены."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q6RXl5VAceEm"
   },
   "source": [
    "net = char_rnn.model\n",
    "checkpoint = {\"n_hidden\": net.n_hidden,\n",
    "              \"n_layers\": net.n_layers,\n",
    "              \"state_dict\": net.state_dict(),\n",
    "              \"tokens\": net.unique_tokens}\n",
    "\n",
    "with open(\"rnn_x_epoch.net\", \"wb\") as f:\n",
    "    torch.save(checkpoint, f)\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2sJhx5iceEm"
   },
   "source": [
    "### Делаем предсказания\n",
    "\n",
    "Сгенерируем текст! Для предсказания продолжения текста мы передаём в сеть последний символ, она предсказывает следующий символ, который мы снова передаем на вход, получаем ещё один предсказанный символ и так далее. Наши прогнозы основаны на категориальном распределении вероятностей по всем возможным символам. Мы можем ограничить число символов на каждом шаге генерации, чтобы сделать получаемый предсказанный текст более разумным, рассматривая только некоторые, наиболее вероятные символы. С одной стороны, такой подход позволит нам рассматривать не только самую вероятную последовательность с точки зрения прогноза модели. С другой стороны, мы будем работать с ограниченным набором сгенерированных вариантов, поэтому избавимся от совсем уж шумовых прогнозов. Узнать больше можно [здесь](https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QEIRW_B2ceEm"
   },
   "source": [
    "def predict_next_char(model: torch.nn.Module, char: str, h: torch.Tensor = None, top_k: int = None) -> Tuple[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Given a character and a model, predicts next character in the sequence\n",
    "        :param model: model that outputs next token probability distribution\n",
    "        :param char: last character of the sequence to continue generation from\n",
    "        :param h: hidden state of the model\n",
    "        :param top_k: number of most probable tokens to be chosen from\n",
    "        :return: tuple of next character and new hidden state\n",
    "        \"\"\"\n",
    "        # tensor inputs\n",
    "        x = torch.tensor([[model.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(model.unique_tokens))\n",
    "        x = x.to(device)\n",
    "\n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # get the output of the model\n",
    "        out, h = model(x, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = torch.nn.functional.softmax(out, dim=1).data.cpu()\n",
    "\n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = torch.arange(len(model.unique_tokens))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch.numpy(), p=p/p.sum())\n",
    "        \n",
    "        return model.int2char[char], h"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OG38j3gQceEm"
   },
   "source": [
    "### Priming и генерирование текста\n",
    "\n",
    "Нужно задать скрытое состояние, чтобы сеть не генерировала произвольные символы. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P9vpB5gRceEm"
   },
   "source": [
    "def sample(model, size, prime=\"The\", top_k=None):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = model.init_hidden(1, device)\n",
    "    for ch in prime:\n",
    "        char, h = predict_next_char(model, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict_next_char(model, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return \"\".join(chars)"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "942mjdQHceEm"
   },
   "source": [
    "### Загрузка чекпоинта и генерация"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Xt9ldUuSceEm"
   },
   "source": [
    "with open(\"rnn_x_epoch.net\", \"rb\") as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint[\"tokens\"], n_hidden=checkpoint[\"n_hidden\"], n_layers=checkpoint[\"n_layers\"])\n",
    "loaded.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said hhe a ha et har hint oe  ans he at et ot ots oed teta tot ee a to as hasttor hh a as a enti os en hond ate ho  on asdtite thin and a en te a hess et an an etit ta so o as hon  ar ho tot tondd eedthe  on essen hi arnsi oen tase esne tostes tees as ans e he hen ar oe  oe  ee has e ee hit ettesd ee atte  a te o a hin  eta etesd astin te as tat ee ethe has at ho  e at hhi  ann anst etet hann totte hhtt ettas oe  ot hand thonts hi  he ansd otet het het hired attestti  an o the tint on har te andte eten hh seet anstas etos o ar oetid a anns hen eseta antte esthe ha  ho  itet ettat eed on oese an het hir a eten e asted a hat hot tat a atta ann thentde ten a an at ann ante et or ae tonn hir his a hho tots ere ta enthan tho etin aret ertat ar het hor ot ton  annd he a tita thass atti es oe eet ot ho as hhats hinn hhrttar hh  he tar tet asdsisd harn es tite ha  he entis an ti hond\n",
      "are ennteste harsennn the tat on he ha e tis ton the a tene aste tan tondnte ar at eren het as hats osnd tetette tors a etetti e o hi e hi onn on tee  ass ar his esetett has he an a ten otedddedd an ho ans etan to hon arnet e aren tond ondnth etirt en onnd as oe hir attensde tans onddtis e ar ho hisns tins at oes o te at tor he te an thond he et ha thonn hh so a e asen at hend ot the  et a arsto e ha to hen hisd enn etore tins ont et as ha ha ans ten tatttont a te he  ireste  enn hi ha eti en and e teete  he to tit har et onddd tottti  ho tars herde an oetit oenn at ost et ti et onsdstas hhe ta hhe hit has onn o etor hit as ete oenn asd e ton tatstate asse herd oe ees e oes thorta ene a thon tare tor hat tee  en at har on o oe ha at asdset hon artiren ti a ot as a e atte to esendte his estenn hos e at at tand e he as tates hhr on he tot tita hhrn at an os are a e hi tho as a hes an o hesdd thot oesthat on tit ont anntet a etar har thonn enssen e hated tis tot oe hes ho e ott hen thed tor hotttis te enti ared a enn totndd etin atnsstan esterte thed ter tirtte as astho ton te ene os ats etosn anssedn \n"
     ]
    }
   ]
  }
 ]
}
