{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 2: Введение в tensorflow\n",
    "\n",
    "На этом семинаре мы познакомимся с классическим фреймворком `tensorflow`, разберёмся с автоматическим дифференцированием, а также обучим нашу первую нейросеть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как устроен tf изнутри?\n",
    "\n",
    "Идея tensorflow заключается в построении так называемых **графов вычислений**, где вершины представляют из себя **тензоры** (многомерные массивы, в точности как `np.array` в знакомом вам `numpy`)вместе с **операции** над этими тензорами, а рёбра -- **передачу данных из одной операции в другую**.\n",
    "\n",
    "**ВНИМАНИЕ!**\n",
    "\n",
    "Вводя каким-нибудь образом тензоры и проводя над ними операции в питоновском коде Вы эти операции **не запускаете**, а лишь **определяете их заранее**, конструируя при этом граф вычислений в памяти. Как запускать эти определённые только что операции для получения результата мы узнаем немного позже.\n",
    "\n",
    "Два основных вида тензоров:\n",
    "\n",
    "1. `tf.placeholder` -- это тензор, который необходимо инициализировать руками **на этапе запуска операции**. Удобно использовать его как мнимый вход нейронной сети, которому мы впоследствии присвоим данные для обучения и тестирования.\n",
    "2. `tf.Variable` -- это тензор-переменная, который имеет некоторую **инициализацию по умолчанию**. Удобно под ним понимать обучаемые параметры нейронных сетей, линейных моделей и т.п.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Когда Вы создаёте плейсхолдер, обязательно необходимо указывать его тип, в данном случае это float32.\n",
    "input_vector = tf.placeholder(\"float32\",)\n",
    "\n",
    "# Опционально Вы можете указать ещё и размерность тензора. \n",
    "# При запуске операций его размерность станет известна, лучше указывать это заранее, это очень полезно для дебага\n",
    "input_vector_with_shape = tf.placeholder(\"float32\", shape=(12,))\n",
    "\n",
    "# Вы также можете указать размерности, которые заранее неизвестны (например, размер батча)\n",
    "# при помощи ключевого слова None\n",
    "input_vector_with_unknown_dim = tf.placeholder(\"float32\", shape=(None, None, 12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Для переменных необходимо подать значение по умолчанию\n",
    "# Тип переменной может быть определён по значению по умолчанию\n",
    "variable_vector = tf.Variable(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ниже мы попробуем поделать какие-нибудь операции над тензорами. Они фактически точно такие же, как в `numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(\"float32\", shape=(12,))\n",
    "y = tf.placeholder(\"float32\", shape=(12,))\n",
    "\n",
    "# Поэлементное сложение, умножение, деление, whatever векторов\n",
    "z = x + y\n",
    "z = x - y\n",
    "z = x * y\n",
    "\n",
    "# Нахождеие среднего и среднего по всем элементам\n",
    "# np.sum -> tf.reduce_sum\n",
    "# np.mean -> tf.reduce_mean\n",
    "s = tf.reduce_sum(x)\n",
    "m = tf.reduce_mean(x)\n",
    "\n",
    "# Математические операции\n",
    "sin_x = tf.sin(x)\n",
    "exp_x = tf.exp(x)\n",
    "\n",
    "# Скалярное произведение\n",
    "z = tf.tensordot(x, y, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список всевозможных трансформаций тензоров мы советуем искать в официальной документации к tensorflow, она очень качественно и подробно описана. В упражнениях мы попробуем давать хинты к операциям, которые необходимо использовать."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow также предоставляет возможность использования уже инициализированных тензоров, в точности как в numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.arange -> tf.range\n",
    "range_12 = tf.range(12)\n",
    "\n",
    "# np.zeros, np.ones -> tf.zeros, tf.ones\n",
    "zeros_12 = tf.zeros(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Упражнение** Попробуйте сами поиграть с тензорами и вычислить значение выражения\n",
    "\n",
    "$$\n",
    "    z = \\cos \\langle \\cfrac{x}{x + y}, \\cfrac{y}{x + y}\\rangle\n",
    "$$\n",
    "\n",
    "Где $\\langle \\cdot, \\cdot \\rangle$ обозначает скалярное произведение векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float32\", shape=(12,))\n",
    "y = tf.placeholder(\"float32\", shape=(12,))\n",
    "\n",
    "z = # your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можете не вникать в происходящее, просто запустите ячейку ниже для самопроверки. Должно вывести нечто близкое к -0.8893267."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(z, {x: np.ones(12), y: 2*np.ones(12)}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Упражнение посложнее**. Попробуйте имплементировать квадратичную функцию потерь между двумя векторами:\n",
    "\n",
    "$$\n",
    "    MSE(x, y) = \\sum_{i=1}^{n} (x_i - y_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float32\")\n",
    "y = tf.placeholder(\"float32\")\n",
    "\n",
    "mse = # Your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тесты для проверки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "for n in [1,5,10,10**3]:\n",
    "    \n",
    "    elems = [np.arange(n),np.arange(n,0,-1), np.zeros(n),\n",
    "             np.ones(n),np.random.random(n),np.random.randint(100,size=n)]\n",
    "    \n",
    "    for el in elems:\n",
    "        for el_2 in elems:\n",
    "            true_mse = np.array(mean_squared_error(el,el_2))\n",
    "            my_mse = sess.run(mse, {x:el, y:el_2})\n",
    "            if not np.allclose(true_mse,my_mse):\n",
    "                print('Wrong result:')\n",
    "                print('mse(%s,%s)' % (el,el_2))\n",
    "                print(\"should be: %f, but your function returned %f\" % (true_mse,my_mse))\n",
    "                sess.close()\n",
    "                raise ValueError(\"Что-то не так\")\n",
    "sess.close()\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## А как запускать операции?\n",
    "\n",
    "Для того, чтобы граф вычислений запустился, необходимо населить все тензоры значениями, то есть положить их в оперативную память (или в память на Вашей видеокарте, если у вас таковая есть =)). Всё общение тензорфлоу с аппаратным памятью происходит через **сессии**.\n",
    "\n",
    "Создавая сессию, Вы резервируете место в памяти под тензоры. Через сессию Вы также можете запускать операции.\n",
    "\n",
    "**Замечание** \n",
    "Ваша запускаемая операция может зависеть от других операций, которые в свою очередь зависят от ещё каких-нибудь, и так далее. Тензорфлоу способен сам выявить какие операции от каких зависят и последовательно их запустить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаём какой-нибудь простой граф вычислений\n",
    "x = tf.ones(12)\n",
    "y = tf.ones(12)\n",
    "z = tf.reduce_sum(x + y)\n",
    "\n",
    "# создаём сессию\n",
    "sess = tf.Session()\n",
    "\n",
    "# Запускаем операцию\n",
    "print(sess.run(z))\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если ваша операция как-то зависит от placeholder'ов, то их необходимо явным образом инициализировать через так называемый **feed dict** -- это словарь, где ключом является ваш плейсхолдер, а значением -- numpy-массив с его значением. Этот словать нужно передать вторым аргументом в функции run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# абсолютно тот же самый код, но уже с плейсхолдерами\n",
    "x = tf.placeholder(\"float32\", (12,))\n",
    "y = tf.placeholder(\"float32\", (12,))\n",
    "z = tf.reduce_sum(x + y)\n",
    "\n",
    "# создаём сессию\n",
    "sess = tf.Session()\n",
    "\n",
    "feed_dict = {x: np.ones(12), y: np.ones(12)}\n",
    "# Запускаем операцию\n",
    "print(sess.run(z, feed_dict=feed_dict))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВНИМАНИЕ** Обязательно закрывайте сессии при помощи `sess.close()`  или пользуйтесь конструкцией `with tf.Session() as sess:` после использования иначе Вы рискуете столкнуться с вопросом \"А почему мой компьютер так медленно работает?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подсчёт градиентов.\n",
    "\n",
    "А теперь перейдём к самой главной функции тф'a, а именно к автоматическому подсчёту производных! Дело в том, что если Вы делаете какие-то поддерживаемые тф'ом операции над тензорами, тф умеет по этим операциям считать производные как производные функций многих переменных.\n",
    "\n",
    "Для подсчёта градиента  $\\cfrac{\\partial y}{\\partial x}$ Вы просто вызываете функцию `tf.gradients(y, x)`. \n",
    "\n",
    "Чтобы убедиться, что эта функция действительно считает градиенты, попробуем построить график произвольной гладкой функции и касательной к ней.\n",
    "\n",
    "Напомним, что уравнение касательной к функции f(x) в точке ($x_0, y_0$) выглядит как \n",
    "$$\n",
    "    y(x) = y'(x_0) \\cdot (x - x_0) + y_0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# определяем функцию\n",
    "x = tf.placeholder(\"float32\")\n",
    "y = tf.sin(x * 5) / (x * 5)\n",
    "\n",
    "# вычисляем наклон и сдвиг графика\n",
    "[slope] = # your code\n",
    "bias = # your code\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    plt.figure(figsize=(10,8))\n",
    "    x_support = np.arange(0, 10, 0.01)\n",
    "    y_res = sess.run(y, feed_dict={x: x_support})\n",
    "    slope_res, bias_res = sess.run([slope, bias], feed_dict={x: 4.})\n",
    "    plt.plot(x_support, y_res, label=\"function\")\n",
    "    plt.plot(x_support, slope_res * x_support + bias_res, label=\"derivative\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Должна получиться касательная к графику функции $\\sin(5x) / (5x)$ в точке x=4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В случае, когда у нас функция y = f(x) имеет многомерный аргумент (да и возвращает она многомерное значение), Вы получите на выходе из tf.gradients также мнгомерный тензор, в котором подсчитана производная каждого элемента $y$ по каждому элементу $x$. Например, если у вас $y$ и $x$ являются одномерными массивами, то на tf.gradients вернёт двумерную матрицу $A$ производных, $A_{ij} = \\cfrac{\\partial y_i}{\\partial x_j}$ (то есть так называемый **Якобиан**). \n",
    "\n",
    "По-правде можно и голову сломать, пытаясь понять, какой размерности будут тензоры после подсчёта градиентов, однако на практике сверх рассмотренных нами примеров практически ничего не бывает. Ну или бывает, но такие случаи мы рассматривать в курсе не будем. =)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный спуск.\n",
    "\n",
    "Подсчёт градиентов необходим нам для прямого применения методов оптимизации. Чтобы нам не приходилось постоянно программировать формулы для вычисления направлений оптимизации в разных методах, создатели tensorflow уже запрогали многое за нас. Нас будут интересовать так называемые **optimizer**'ы. Эти объекты умеют производить шаги градиентного спуска и оптимизировать произвольные функции. Попробуем с ними поиграть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_guess = tf.Variable(np.zeros(2,dtype='float32'))\n",
    "y_true = tf.range(1,3,dtype='float32')\n",
    "\n",
    "loss = tf.reduce_mean((y_guess - y_true + tf.random_normal([2]))**2) \n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss,var_list=y_guess)\n",
    "\n",
    "#same, but more detailed:\n",
    "#updates = [[tf.gradients(loss,y_guess)[0], y_guess]]\n",
    "#optimizer = tf.train.MomentumOptimizer(0.01,0.9).apply_gradients(updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "guesses = [sess.run(y_guess)]\n",
    "\n",
    "for _ in range(250):\n",
    "    sess.run(optimizer)\n",
    "    guesses.append(sess.run(y_guess))\n",
    "    \n",
    "    clear_output(True)\n",
    "    plt.plot(*zip(*guesses),marker='.')\n",
    "    plt.scatter(*sess.run(y_true),c='red')\n",
    "    plt.show()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Собираем всё воедино: моя первая нейросеть.\n",
    "\n",
    "Нейронные сети обучаются при помощи алгоритма обратного распространения ошибки (англ. backpropagation или просто backprop). Хорошая визуализация работы этого алгоритма приведена [тут](https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/). На следующем семинаре мы в больших подробностях рассмотрим принцип его работы и даже запрограммируем его вручную при помощи библиотеки numpy, а пока что всё, что нам нужно знать -- это то, что `tensorflow` умеет **эффективно вычислять градиенты** при помощи бэкпропа.\n",
    "\n",
    "Ниже уже идёт более приближенный к реальности код на тензорфлоу. Попробуйте разобраться с ним сами. =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "mnist = load_digits()\n",
    "\n",
    "X,y = mnist.data, mnist.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)\n",
    "print(\"y [shape - %s]:\" % (str(y.shape)), y[:10])\n",
    "print(\"X [shape - %s]:\" % (str(X.shape)))\n",
    "n_labels = len(np.unique(y))\n",
    "\n",
    "batch_size = 32\n",
    "n_batch_train = len(X_train) // batch_size\n",
    "n_batch_test = len(X_test) // batch_size\n",
    "\n",
    "# Входные данные\n",
    "X_input = # YOUR CODE\n",
    "y_input = # YOUR CODE\n",
    "\n",
    "# Архитектура\n",
    "# YOUR CODE \n",
    "\n",
    "# Лосс\n",
    "loss = # YOUR CODE\n",
    "\n",
    "# Запускаемая операция одного шаша градиентного спуска.\n",
    "train_op = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(10):\n",
    "        y_pred_array = []\n",
    "        for b in range(n_batch_train):\n",
    "            _, loss_ = sess.run([train_op, loss], feed_dict={X_input: X_train[b*batch_size:(b+1)*batch_size],\n",
    "                                                             y_input: y_train[b*batch_size:(b+1)*batch_size]\n",
    "                                                             })\n",
    "        for b in range(n_batch_test + 1):\n",
    "            loss_, y_pred = sess.run([loss, predictions], feed_dict={X_input: X_test[b*batch_size:(b+1)*batch_size],\n",
    "                                          y_input: y_test[b*batch_size:(b+1)*batch_size]\n",
    "                                         })\n",
    "            y_pred_array += np.argmax(y_pred, axis=-1).tolist()\n",
    "        print(accuracy_score(y_pred_array, y_test))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
